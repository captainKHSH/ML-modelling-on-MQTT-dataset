{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e19a08cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SQLContext\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "appName = \"Project_2\"\n",
    "master = \"local\"\n",
    "\n",
    "# Create Configuration object for Spark.\n",
    "conf = pyspark.SparkConf()\\\n",
    "    .set('spark.driver.host','127.0.0.1')\\\n",
    "    .setAppName(appName)\\\n",
    "    .setMaster(master)\n",
    "\n",
    "# Create Spark Context with the new configurations rather than relying on the default one\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "# You need to create SQL Context to conduct some database operations like what we will see later.\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# If you have SQL context, you create the session from the Spark Context\n",
    "spark = sqlContext.sparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c66842ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from csv to a dataframe on a local machine. \n",
    "# header=False means the first row is not a header \n",
    "# sep=',' means the column are seperated using ','\n",
    "\n",
    "\n",
    "df_train = spark.read.csv(\"train70_reduced.csv\",header=True, inferSchema= True)\n",
    "df_test = spark.read.csv(\"test30_reduced.csv\",header=True, inferSchema= True)\n",
    "df_train = df_train.withColumn(\"dataset\", lit(\"train\"))\n",
    "df_test = df_test.withColumn(\"dataset\", lit(\"test\"))\n",
    "df = df_train.union(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0df0e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_properties={}\n",
    "db_properties['username']=\"postgres\"\n",
    "db_properties['password']=\"Natkanvij@22\"\n",
    "db_properties['url']= \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "\n",
    "# I have kept the table name as intrusion2\n",
    "db_properties['table']=\"MQTT\" \n",
    "db_properties['driver']=\"org.postgresql.Driver\"\n",
    "\n",
    "df.write.format(\"jdbc\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"url\", db_properties['url'])\\\n",
    ".option(\"dbtable\", db_properties['table'])\\\n",
    ".option(\"user\", db_properties['username'])\\\n",
    ".option(\"password\", db_properties['password'])\\\n",
    ".option(\"Driver\", db_properties['driver'])\\\n",
    ".save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9573e7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " tcp.flags                  | 0x00000018 \n",
      " tcp.time_delta             | 0.998867   \n",
      " tcp.len                    | 10         \n",
      " mqtt.conack.flags          | 0          \n",
      " mqtt.conack.flags.reserved | 0.0        \n",
      " mqtt.conack.flags.sp       | 0.0        \n",
      " mqtt.conack.val            | 0.0        \n",
      " mqtt.conflag.cleansess     | 0.0        \n",
      " mqtt.conflag.passwd        | 0.0        \n",
      " mqtt.conflag.qos           | 0.0        \n",
      " mqtt.conflag.reserved      | 0.0        \n",
      " mqtt.conflag.retain        | 0.0        \n",
      " mqtt.conflag.uname         | 0.0        \n",
      " mqtt.conflag.willflag      | 0.0        \n",
      " mqtt.conflags              | 0          \n",
      " mqtt.dupflag               | 0.0        \n",
      " mqtt.hdrflags              | 0x00000030 \n",
      " mqtt.kalive                | 0.0        \n",
      " mqtt.len                   | 8.0        \n",
      " mqtt.msg                   | 32         \n",
      " mqtt.msgid                 | 0.0        \n",
      " mqtt.msgtype               | 3.0        \n",
      " mqtt.proto_len             | 0.0        \n",
      " mqtt.protoname             | 0          \n",
      " mqtt.qos                   | 0.0        \n",
      " mqtt.retain                | 0.0        \n",
      " mqtt.sub.qos               | 0.0        \n",
      " mqtt.suback.qos            | 0.0        \n",
      " mqtt.ver                   | 0.0        \n",
      " mqtt.willmsg               | 0.0        \n",
      " mqtt.willmsg_len           | 0.0        \n",
      " mqtt.willtopic             | 0.0        \n",
      " mqtt.willtopic_len         | 0.0        \n",
      " target                     | legitimate \n",
      " dataset                    | train      \n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- tcp.flags: string (nullable = true)\n",
      " |-- tcp.time_delta: double (nullable = true)\n",
      " |-- tcp.len: integer (nullable = true)\n",
      " |-- mqtt.conack.flags: string (nullable = true)\n",
      " |-- mqtt.conack.flags.reserved: double (nullable = true)\n",
      " |-- mqtt.conack.flags.sp: double (nullable = true)\n",
      " |-- mqtt.conack.val: double (nullable = true)\n",
      " |-- mqtt.conflag.cleansess: double (nullable = true)\n",
      " |-- mqtt.conflag.passwd: double (nullable = true)\n",
      " |-- mqtt.conflag.qos: double (nullable = true)\n",
      " |-- mqtt.conflag.reserved: double (nullable = true)\n",
      " |-- mqtt.conflag.retain: double (nullable = true)\n",
      " |-- mqtt.conflag.uname: double (nullable = true)\n",
      " |-- mqtt.conflag.willflag: double (nullable = true)\n",
      " |-- mqtt.conflags: string (nullable = true)\n",
      " |-- mqtt.dupflag: double (nullable = true)\n",
      " |-- mqtt.hdrflags: string (nullable = true)\n",
      " |-- mqtt.kalive: double (nullable = true)\n",
      " |-- mqtt.len: double (nullable = true)\n",
      " |-- mqtt.msg: string (nullable = true)\n",
      " |-- mqtt.msgid: double (nullable = true)\n",
      " |-- mqtt.msgtype: double (nullable = true)\n",
      " |-- mqtt.proto_len: double (nullable = true)\n",
      " |-- mqtt.protoname: string (nullable = true)\n",
      " |-- mqtt.qos: double (nullable = true)\n",
      " |-- mqtt.retain: double (nullable = true)\n",
      " |-- mqtt.sub.qos: double (nullable = true)\n",
      " |-- mqtt.suback.qos: double (nullable = true)\n",
      " |-- mqtt.ver: double (nullable = true)\n",
      " |-- mqtt.willmsg: double (nullable = true)\n",
      " |-- mqtt.willmsg_len: double (nullable = true)\n",
      " |-- mqtt.willtopic: double (nullable = true)\n",
      " |-- mqtt.willtopic_len: double (nullable = true)\n",
      " |-- target: string (nullable = true)\n",
      " |-- dataset: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_read = sqlContext.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", db_properties['url'])\\\n",
    "    .option(\"dbtable\", db_properties['table'])\\\n",
    "    .option(\"user\", db_properties['username'])\\\n",
    "    .option(\"password\", db_properties['password'])\\\n",
    "    .option(\"Driver\", db_properties['driver'])\\\n",
    "    .load()\n",
    "\n",
    "df_read.show(1, vertical=True)\n",
    "df_read.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a64f7f",
   "metadata": {},
   "source": [
    "Renaming coloumns with . to _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5047d8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def replace_dot_with_underscore(data_df):\n",
    "    # Replace '.' with '_' in column names\n",
    "    for col_name in data_df.columns:\n",
    "        new_col_name = col_name.replace('.', '_')\n",
    "        data_df = data_df.withColumnRenamed(col_name, new_col_name)\n",
    "    return data_df\n",
    "renamed_columns_df = replace_dot_with_underscore(df_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "721caec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tcp_flags: string (nullable = true)\n",
      " |-- tcp_time_delta: double (nullable = true)\n",
      " |-- tcp_len: integer (nullable = true)\n",
      " |-- mqtt_conack_flags: string (nullable = true)\n",
      " |-- mqtt_conack_flags_reserved: double (nullable = true)\n",
      " |-- mqtt_conack_flags_sp: double (nullable = true)\n",
      " |-- mqtt_conack_val: double (nullable = true)\n",
      " |-- mqtt_conflag_cleansess: double (nullable = true)\n",
      " |-- mqtt_conflag_passwd: double (nullable = true)\n",
      " |-- mqtt_conflag_qos: double (nullable = true)\n",
      " |-- mqtt_conflag_reserved: double (nullable = true)\n",
      " |-- mqtt_conflag_retain: double (nullable = true)\n",
      " |-- mqtt_conflag_uname: double (nullable = true)\n",
      " |-- mqtt_conflag_willflag: double (nullable = true)\n",
      " |-- mqtt_conflags: string (nullable = true)\n",
      " |-- mqtt_dupflag: double (nullable = true)\n",
      " |-- mqtt_hdrflags: string (nullable = true)\n",
      " |-- mqtt_kalive: double (nullable = true)\n",
      " |-- mqtt_len: double (nullable = true)\n",
      " |-- mqtt_msg: string (nullable = true)\n",
      " |-- mqtt_msgid: double (nullable = true)\n",
      " |-- mqtt_msgtype: double (nullable = true)\n",
      " |-- mqtt_proto_len: double (nullable = true)\n",
      " |-- mqtt_protoname: string (nullable = true)\n",
      " |-- mqtt_qos: double (nullable = true)\n",
      " |-- mqtt_retain: double (nullable = true)\n",
      " |-- mqtt_sub_qos: double (nullable = true)\n",
      " |-- mqtt_suback_qos: double (nullable = true)\n",
      " |-- mqtt_ver: double (nullable = true)\n",
      " |-- mqtt_willmsg: double (nullable = true)\n",
      " |-- mqtt_willmsg_len: double (nullable = true)\n",
      " |-- mqtt_willtopic: double (nullable = true)\n",
      " |-- mqtt_willtopic_len: double (nullable = true)\n",
      " |-- target: string (nullable = true)\n",
      " |-- dataset: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "renamed_columns_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1f7ba35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length for 'train' dataset: 31.435725201384873\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "\n",
    "# Filter the DataFrame to select only rows where 'dataset' is 'train'\n",
    "train_df = renamed_columns_df.filter(renamed_columns_df['dataset'] == 'train')\n",
    "\n",
    "# Calculate the average of the 'len' column for the 'train' dataset\n",
    "average_length = train_df.agg(avg('mqtt_len')).collect()[0][0]\n",
    "\n",
    "# Print the average length\n",
    "print(\"Average length for 'train' dataset:\", average_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecf7b77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|    target|average_tcp_length|\n",
      "+----------+------------------+\n",
      "|   slowite|3.9993479678330797|\n",
      "|bruteforce|3.9871043376318873|\n",
      "|     flood|13313.415986949429|\n",
      "| malformed| 20.97491761259612|\n",
      "|       dos|312.65759830457716|\n",
      "|legitimate| 7.776101001432345|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "average_lengths = renamed_columns_df.groupBy('target').agg(avg('tcp_len').alias('average_tcp_length'))\n",
    "df = renamed_columns_df\n",
    "average_lengths.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d94faf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dense_rank, desc\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "def get_X_most_frequent_tcp_flags(x):\n",
    "    distinct_flags = df.select('tcp_flags').distinct().count()\n",
    "    if x > distinct_flags:\n",
    "        print(f\"There are only {distinct_flags} distinct TCP flags in this dataset. Showing all {distinct_flags} TCP flags ranked with dense ranking.\")\n",
    "        x = distinct_flags\n",
    "    window_spec = Window.partitionBy().orderBy(desc('count'))\n",
    "    ranked_df = df.groupBy('tcp_flags').count().orderBy(desc('count')).withColumn(\"dense_rank\", dense_rank().over(window_spec))\n",
    "    result = ranked_df.filter(col(\"dense_rank\") <= x)\n",
    "    result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fb8f602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+\n",
      "| tcp_flags| count|dense_rank|\n",
      "+----------+------+----------+\n",
      "|0x00000018|183076|         1|\n",
      "|0x00000010|134547|         2|\n",
      "|0x00000011|  4198|         3|\n",
      "|0x00000002|  3372|         4|\n",
      "|0x00000012|  3372|         4|\n",
      "+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_X_most_frequent_tcp_flags(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6fe558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "renamed_columns_df = renamed_columns_df.withColumn(\"target\", when(renamed_columns_df[\"target\"] == \"dos\", \"denial-of-service\").otherwise(renamed_columns_df[\"target\"]))\n",
    "renamed_columns_df = renamed_columns_df.withColumn(\"target\", when(renamed_columns_df[\"target\"] == \"bruteforce\", \"brute-force\").otherwise(renamed_columns_df[\"target\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0e8a0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Targets we have from the dataset:\n",
      "slowite\n",
      "flood\n",
      "brute-force\n",
      "malformed\n",
      "denial-of-service\n",
      "legitimate\n"
     ]
    }
   ],
   "source": [
    "unique_target_types = renamed_columns_df.select('target').distinct()\n",
    "\n",
    "# Collect the unique values and convert them to a list\n",
    "unique_target_types_list = [row.target for row in unique_target_types.collect()]\n",
    "\n",
    "# Print the unique protocol types\n",
    "print(\"Unique Targets we have from the dataset:\")\n",
    "for target in unique_target_types_list:\n",
    "    print(target)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "246dc12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import socket\n",
    "#Initialize Your Parameters here - Keep the variable values as is for the ones you can't find on the Confluent-Kafka connection \n",
    "KAFKA_CONFIG = {\n",
    "    \"bootstrap.servers\":\"pkc-lzvrd.us-west4.gcp.confluent.cloud:9092\",\n",
    "    \"security.protocol\":\"SASL_SSL\",\n",
    "    \"sasl.mechanisms\":\"PLAIN\",\n",
    "    \"sasl.username\":\"LKGBAJ3FAQY7XT3O\",\n",
    "    \"sasl.password\":\"FdsLkF9Cec8qkthIz9EAny8whhp9dZ9Wa0/YuBQZa2JjEsF/61KnaGkkpK7VW9fk\",\n",
    "    \"session.timeout.ms\":\"45000\",\n",
    "    \"group.id\":\"python-group-1\",\n",
    "    'auto.offset.reset': 'smallest',\n",
    "    'client.id': socket.gethostname()\n",
    "}\n",
    "\n",
    "# Update your topic name\n",
    "topic_name = \"topic_0\"\n",
    "producer = Producer(KAFKA_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336b152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import time\n",
    "\n",
    "# We are searching for Analytics in the news\n",
    "feed_url = \"https://news.google.com/rss/search?q=popular+cyber+attacks\"\n",
    "def extract_news_feed(feed_url, runtime_minutes):\n",
    "    feed = feedparser.parse(feed_url)\n",
    "    articles = []\n",
    "    extracted_articles = set()\n",
    "    start_time = time.time()\n",
    "    end_time = start_time + (runtime_minutes * 60)  # Convert minutes to seconds\n",
    "    while time.time() < end_time:\n",
    "        for entry in feed.entries:\n",
    "            link = entry.link\n",
    "            title = entry.title.encode('ascii', 'ignore').decode()\n",
    "            unique_id = f'{link}-{title}'\n",
    "            if unique_id in extracted_articles:\n",
    "                continue\n",
    "            extracted_articles.add(unique_id)\n",
    "            article_data = {\"title\": title, \"link\": link}\n",
    "            if article_data is not None:\n",
    "                producer.produce(topic_name, key=article_data[\"title\"], value=article_data[\"link\"])\n",
    "        producer.flush()\n",
    "\n",
    "extract_news_feed(feed_url, runtime_minutes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab2c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer\n",
    "from pyspark.sql.types import *\n",
    "import string\n",
    "\n",
    "\n",
    "# Clean the punctation by making a translation table that maps punctations to empty strings\n",
    "translator = str.maketrans(\"\", \"\", string.punctuation.replace('-', ''))\n",
    "\n",
    "\n",
    "emp_RDD = spark.sparkContext.emptyRDD()\n",
    "# Defining the schema of the DataFrame\n",
    "columns = StructType([StructField('key', StringType(), False),\n",
    "                      StructField('value', StringType(), False)])\n",
    "\n",
    "# Creating an empty DataFrame\n",
    "df = spark.createDataFrame(data=emp_RDD,\n",
    "                                   schema=columns)\n",
    " \n",
    "# Printing the DataFrame with no data\n",
    "df.show()\n",
    "\n",
    "consumer = Consumer(KAFKA_CONFIG)\n",
    "consumer.subscribe([topic_name])\n",
    "\n",
    "try:\n",
    "    i = 0\n",
    "    while i < 5:\n",
    "        msg = consumer.poll(timeout=1.0)\n",
    "        if msg is None:\n",
    "            i = i + 1\n",
    "            print(\"Waiting...\")\n",
    "            continue\n",
    "        if msg is not None:\n",
    "            key = msg.key().decode('utf-8').lower().translate(translator)\n",
    "            #print(key)\n",
    "            cleaned_key = \" \".join(key.split())\n",
    "            value = msg.value().decode('utf-8')\n",
    "            added_row = [[cleaned_key,value]]\n",
    "            added_df = spark.createDataFrame(added_row, columns)\n",
    "            df = df.union(added_df)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    consumer.close()\n",
    "    df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4782bd2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc79c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# import nltk \n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# stop_words = nltk.corpus.stopwords.words('english')\n",
    "streamed_data = df.withColumn('word', explode(split(col('key'), ' '))) \\\n",
    "                .filter(col('word').isin(unique_target_types_list)) \\\n",
    "                .groupBy('word') \\\n",
    "                .count() \\\n",
    "                .sort('count', ascending=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "streamed_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada206f",
   "metadata": {},
   "source": [
    "renaming the target values to the original type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "renamed_columns_df = renamed_columns_df.withColumn(\"target\", when(renamed_columns_df[\"target\"] == \"denial-of-service\", \"dos\").otherwise(renamed_columns_df[\"target\"]))\n",
    "renamed_columns_df = renamed_columns_df.withColumn(\"target\", when(renamed_columns_df[\"target\"] == \"brute-force\", \"bruteforce\").otherwise(renamed_columns_df[\"target\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127b9d7",
   "metadata": {},
   "source": [
    "Task- III Machine Learning Modeling\n",
    "\n",
    "Feature Engineering\n",
    "\n",
    "*Check for Null and NA values in the coloumns*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7374fda1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5dfae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "null_df = renamed_columns_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) \\\n",
    "                        for c in renamed_columns_df.columns])\n",
    "\n",
    "null_df.show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b470f434",
   "metadata": {},
   "source": [
    "There are no null/NA values in the coloumns of our dataset. Thus, we do not need to do imputation for null/Na values.\n",
    "\n",
    "*Checking duplicate rows*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff3f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count = renamed_columns_df.count()\n",
    "print(f\"Current number of rows:{row_count}\")\n",
    "\n",
    "df = renamed_columns_df.dropDuplicates()\n",
    "row_count = df.count()\n",
    "print(f\"After eliminating duplicates, current number of rows:{row_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb6f7ee",
   "metadata": {},
   "source": [
    "*Classifying our variables*\n",
    "\n",
    "To classify our variables lets first check the unique values in the coloumns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bd6955",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in renamed_columns_df.columns:\n",
    "    unique_values = df.select(column).distinct()\n",
    "    print(f\"Column: {column}\")\n",
    "    unique_values.show(truncate=False)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd5b44b",
   "metadata": {},
   "source": [
    "We can see that there are coloumns with only one unique value. Let's drop these coloumns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cff47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"mqtt_conack_flags_reserved\",\"mqtt_conack_flags_sp\",\"mqtt_conflag_qos\",\"mqtt_conflag_reserved\",\n",
    "             \"mqtt_conflag_retain\",\"mqtt_conflag_willflag\",\"mqtt_sub_qos\",\"mqtt_suback_qos\",\"mqtt_willmsg\",\n",
    "             \"mqtt_willmsg_len\",\"mqtt_willtopic\",\"mqtt_willtopic_len\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1168219b",
   "metadata": {},
   "source": [
    "Now, there are binary coloumns which has values as strings, values other than 0 and 1. Therefore, lets encode these coloumns with binary values 0 and 1. Also we will cast these coloumns to datatype double."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b329602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Note when a coloumn is encoded we will drop the original coloumn.\n",
    "conversion_map1 = {\"0\": 0, \"0x00000000\": 1}\n",
    "conversion_map2 = {\"train\": 0, \"test\": 1}\n",
    "conversion_map3 = {\"0.0\": 0, \"5.0\": 1}\n",
    "conversion_map4 = {\"0.0\": 0, \"4.0\": 1}\n",
    "conversion_map5 = {\"MQTT\": 0, \"0\": 1}\n",
    "\n",
    "# Note in the below code \"mqtt_conack_flags\"] == \"0\" and df[\"dataset\"] == \"train\" is used as its dataype is a string\n",
    "df_binary = df.withColumn(\"mqtt_conack_flags_encoded_binary\", when(df[\"mqtt_conack_flags\"] == \"0\", conversion_map1[\"0\"]).otherwise(conversion_map1[\"0x00000000\"])).drop('mqtt_conack_flags')\n",
    "df_binary = df_binary.withColumn(\"dataset_encoded_binary\", when(df[\"dataset\"] == \"train\", conversion_map2[\"train\"]).otherwise(conversion_map2[\"test\"])).drop('dataset')\n",
    "\n",
    "# Note in the below code df[\"mqtt_conack_val\"] == 0.0 and df[\"mqtt_proto_len\"] == 0.0 is used as its dataype is a double\n",
    "df_binary = df_binary.withColumn(\"mqtt_conack_val_encoded_binary\", when(df[\"mqtt_conack_val\"] == 0.0, conversion_map3[\"0.0\"]).otherwise(conversion_map3[\"5.0\"])).drop('mqtt_conack_val')\n",
    "df_binary = df_binary.withColumn(\"mqtt_proto_len_encoded_binary\", when(df[\"mqtt_proto_len\"] == 0.0, conversion_map4[\"0.0\"]).otherwise(conversion_map4[\"4.0\"])).drop('mqtt_proto_len')\n",
    "\n",
    "# Note in the below code [\"mqtt_protoname\"] == MQTT is used as its dataype is a string\n",
    "df_binary = df_binary.withColumn(\"mqtt_protoname_encoded_binary\", when(df[\"mqtt_protoname\"] == 'MQTT', conversion_map5[\"MQTT\"]).otherwise(conversion_map5[\"0\"])).drop('mqtt_protoname')\n",
    "\n",
    "# Casting string datatypes\n",
    "df_binary = df_binary.withColumn('mqtt_conack_flags_encoded_binary', col('mqtt_conack_flags_encoded_binary').cast('double'))\n",
    "df_binary = df_binary.withColumn('dataset_encoded_binary', col('dataset_encoded_binary').cast('double'))\n",
    "df_binary = df_binary.withColumn('mqtt_protoname_encoded_binary', col('mqtt_protoname_encoded_binary').cast('double'))\n",
    "\n",
    "\n",
    "df_binary.printSchema()\n",
    "df = df_binary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84c44dd",
   "metadata": {},
   "source": [
    "Lets investigate more on the coloumn mqtt_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5350fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_count = df.select(countDistinct(col('mqtt_msg')).alias('unique_values_count')).collect()[0]['unique_values_count']\n",
    "\n",
    "print(f\"Number of unique values in 'your_column': {unique_values_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b96332",
   "metadata": {},
   "source": [
    "As we can see this coloumn has roughly half unique values with very large individual dataset numeric value, this coloumn will cause the pipeline to fail. hence dropping this coloumn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aceb95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"mqtt_msg\")\n",
    "df.printSchema()\n",
    "\n",
    "num_columns = len(df.columns)\n",
    "\n",
    "# Print the number of columns\n",
    "print(\"Number of columns:\", num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f8cb5",
   "metadata": {},
   "source": [
    "Now, lets classify our variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a04c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_cols = ['tcp_flags','mqtt_conflags','mqtt_hdrflags']\n",
    "continuous_cols = ['tcp_time_delta','tcp_len','mqtt_len','mqtt_msgid','mqtt_kalive','mqtt_msgtype' ]\n",
    "binary_cols = ['mqtt_protoname_encoded_binary','mqtt_proto_len_encoded_binary','mqtt_conack_val_encoded_binary','dataset_encoded_binary','mqtt_conack_flags_encoded_binary','mqtt_conflag_cleansess','mqtt_conflag_passwd','mqtt_conflag_uname','mqtt_dupflag', 'mqtt_qos','mqtt_retain','mqtt_ver']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64d2995",
   "metadata": {},
   "source": [
    "Summary table for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454eeb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.summary().show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed41f55",
   "metadata": {},
   "source": [
    "Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f9cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [feature[0] for feature in df.dtypes if feature[1] in ('int','double')]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Extract data and convert them into Pandas for visualization\n",
    "converted_data = df[numeric_features].toPandas()\n",
    "\n",
    "figure = plt.boxplot(converted_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b386a9",
   "metadata": {},
   "source": [
    "Lets see outliers and check if we need to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ecae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def column_add(a,b):\n",
    "     return  a.__add__(b)\n",
    "    \n",
    "def find_outliers(df):\n",
    "    # Identifying the numerical columns in a spark dataframe\n",
    "    numeric_columns = [column[0] for column in df.dtypes if column[1]=='int']\n",
    "\n",
    "    # Using the `for` loop to create new columns by identifying the outliers for each feature\n",
    "    for column in numeric_columns:\n",
    "\n",
    "        less_Q1 = 'less_Q1_{}'.format(column)\n",
    "        more_Q3 = 'more_Q3_{}'.format(column)\n",
    "        Q1 = 'Q1_{}'.format(column)\n",
    "        Q3 = 'Q3_{}'.format(column)\n",
    "\n",
    "        # Q1 : First Quartile ., Q3 : Third Quartile\n",
    "        Q1 = df.approxQuantile(column,[0.25],relativeError=0)\n",
    "        Q3 = df.approxQuantile(column,[0.75],relativeError=0)\n",
    "        \n",
    "        # IQR : Inter Quantile Range\n",
    "        # We need to define the index [0], as Q1 & Q3 are a set of lists., to perform a mathematical operation\n",
    "        # Q1 & Q3 are defined seperately so as to have a clear indication on First Quantile & 3rd Quantile\n",
    "        IQR = Q3[0] - Q1[0]\n",
    "        \n",
    "        #selecting the data, with -1.5*IQR to + 1.5*IQR., where param = 1.5 default value\n",
    "        less_Q1 =  Q1[0] - 1.5*IQR\n",
    "        more_Q3 =  Q3[0] + 1.5*IQR\n",
    "        \n",
    "        isOutlierCol = 'is_outlier_{}'.format(column)\n",
    "        \n",
    "        df = df.withColumn(isOutlierCol,when((df[column] > more_Q3) | (df[column] < less_Q1), 1).otherwise(0))\n",
    "    \n",
    "\n",
    "    # Selecting the specific columns which we have added above, to check if there are any outliers\n",
    "    selected_columns = [column for column in df.columns if column.startswith(\"is_outlier\")]\n",
    "    # Adding all the outlier columns into a new colum \"total_outliers\", to see the total number of outliers\n",
    "    df = df.withColumn('total_outliers',reduce(column_add, ( df[col] for col in  selected_columns)))\n",
    "\n",
    "    # Dropping the extra columns created above, just to create nice dataframe., without extra columns\n",
    "    df = df.drop(*[column for column in df.columns if column.startswith(\"is_outlier\")])\n",
    "\n",
    "    return df\n",
    "\n",
    "df_with_outlier_handling = find_outliers(df)\n",
    "df_with_outlier_handling.show(1, vertical=True)\n",
    "\n",
    "df_with_outlier_handling.groupby(\"total_outliers\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec2f601",
   "metadata": {},
   "source": [
    "*As there are no rows with more than 3 outliers, we will not drop any rows on the basis of outliers.*\n",
    "\n",
    "Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4887d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [col for col, dtype in df.dtypes if dtype in ['double', 'int']]\n",
    "numeric_df = df.select(numeric_columns)\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = numeric_df.toPandas().corr()\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980057de",
   "metadata": {},
   "source": [
    "List of correlated columns that needs to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fad186",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_col = ['mqtt_proto_len_encoded_binary', 'mqtt_protoname_encoded_binary','mqtt_conflag_uname','mqtt_qos','mqtt_len','mqtt_ver']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5033365b",
   "metadata": {},
   "source": [
    "Now, let's handle further feature engineering steps including removing correlated columns, one hot encoding, vectorizing the features and outcomes in our pipeline transformer setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d989823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.ml import Pipeline,Transformer\n",
    "from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "normal = ['normal']\n",
    "slowite = ['slowite']\n",
    "brute_force = ['bruteforce']\n",
    "flood = ['flood']\n",
    "malformed = ['malformed']\n",
    "dos = ['dos']\n",
    "legitimate = ['legitimate']\n",
    "\n",
    "class OutcomeCreater(Transformer): # this defines a transformer that creates the outcome column\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "#         label_to_binary = udf(lambda name: 0.0 if name == 'normal' else 1.0 if namdose in slowite else 2.0 if name in brute_force else 3.0 if name in flood else 4.0 if name in malformed else 5.0 if name in denial_of_service else 6.0 if name in legitimate else 7.0)  # Unknown category or any other category not listed\n",
    "#         output_df = dataset.withColumn('outcome', label_to_binary(col('dataset'))).drop(\"dataset\")  \n",
    "#         output_df = output_df.withColumn('outcome', col('outcome').cast(DoubleType()))\n",
    "        label_to_binary = udf(lambda name: 0.0 if name == 'normal' else 1.0 if name == 'slowite' else 2.0 if name == 'bruteforce' else 3.0 if name == 'flood' else 4.0 if name == 'malformed' else 5.0 if name == 'dos' else 6.0 if name == 'legitimate' else -1.0)  # Unknown category or any other category not listed\n",
    "        output_df = dataset.withColumn('outcome', label_to_binary(col('target'))).drop(\"target\")\n",
    "        output_df = output_df.withColumn('outcome', col('outcome').cast(DoubleType()))\n",
    "#         output_df = output_df.drop('dataset')\n",
    "        output_df = output_df.drop('mqtt_msg')\n",
    "        return output_df\n",
    "\n",
    "class FeatureTypeCaster(Transformer): # this transformer will cast the columns as appropriate types  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in binary_cols + continuous_cols:\n",
    "            output_df = output_df.withColumn(col_name,col(col_name).cast(DoubleType()))\n",
    "\n",
    "        return output_df\n",
    "class ColumnDropper(Transformer): # this transformer drops unnecessary columns\n",
    "    def __init__(self, columns_to_drop = None):\n",
    "        super().__init__()\n",
    "        self.columns_to_drop=columns_to_drop\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in self.columns_to_drop:\n",
    "            output_df = output_df.drop(col_name)\n",
    "        return output_df\n",
    "\n",
    "def get_preprocess_pipeline():\n",
    "    # Stage where columns are casted as appropriate types\n",
    "#     columns_to_impute = [\"tcp_time_delta\",\"tcp_len\",\"mqtt_conack_val\",\"mqtt_conflag_cleansess\",\"mqtt_conflag_passwd\",\"mqtt_conflag_uname\",\"mqtt_dupflag\",\"mqtt_kalive\",\"mqtt_len\",\"mqtt_msgid\",\"mqtt_msgtype\",\"mqtt_proto_len\",\"mqtt_qos\",\"mqtt_retain\",\"mqtt_ver\"]\n",
    "#     imputer1 = Imputer(strategy=\"median\", inputCols=columns_to_impute, outputCols=columns_to_impute)\n",
    "    stage_typecaster = FeatureTypeCaster()\n",
    "\n",
    "    # Stage where nominal columns are transformed to index columns using StringIndexer\n",
    "    nominal_id_cols = [x+\"_index\" for x in nominal_cols]\n",
    "    nominal_onehot_cols = [x+\"_encoded\" for x in nominal_cols]\n",
    "    stage_nominal_indexer = StringIndexer(inputCols = nominal_cols, outputCols = nominal_id_cols )\n",
    "\n",
    "    # Stage where the index columns are further transformed using OneHotEncoder\n",
    "    stage_nominal_onehot_encoder = OneHotEncoder(inputCols=nominal_id_cols, outputCols=nominal_onehot_cols)\n",
    "\n",
    "    # Stage where all relevant features are assembled into a vector (and dropping a few)\n",
    "    feature_cols = continuous_cols+binary_cols+nominal_onehot_cols\n",
    "    corelated_cols_to_remove = []\n",
    "    for col_name in corelated_cols_to_remove:\n",
    "        feature_cols.remove(col_name)\n",
    "    stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vectorized_features\", handleInvalid=\"keep\")\n",
    "\n",
    "    # Stage where we scale the columns\n",
    "    stage_scaler = StandardScaler(inputCol= 'vectorized_features', outputCol= 'features')\n",
    "\n",
    "    # Stage for creating the outcome column representing whether there is attack \n",
    "    stage_outcome = OutcomeCreater()\n",
    "\n",
    "    # Removing all unnecessary columbs, only keeping the 'features' and 'outcome' columns\n",
    "    stage_column_dropper = ColumnDropper(columns_to_drop = nominal_cols+nominal_id_cols+\n",
    "        nominal_onehot_cols+ binary_cols + continuous_cols + ['vectorized_features'])\n",
    "    # Connect the columns into a pipeline\n",
    "    pipeline = Pipeline(stages=[stage_typecaster,stage_nominal_indexer,stage_nominal_onehot_encoder,\n",
    "        stage_vector_assembler,stage_scaler,stage_outcome,stage_column_dropper])\n",
    "    return pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e9e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"SystemsToolChains\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e73e8b",
   "metadata": {},
   "source": [
    "Load the training and test dataframe using the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = df.columns\n",
    "split_column = 'dataset_encoded_binary'\n",
    "\n",
    "train_set = df.filter(col(split_column) == '0').toDF(*column_names)\n",
    "test_set = df.filter(col(split_column) == '1').toDF(*column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433aee9b",
   "metadata": {},
   "source": [
    "Creating a preprocess pipeline for train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994028e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline = get_preprocess_pipeline()\n",
    "preprocess_pipeline_model = preprocess_pipeline.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45208e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_df = preprocess_pipeline_model.transform(train_set)\n",
    "pipeline_df_test = preprocess_pipeline_model.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac4f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_df.printSchema()\n",
    "pipeline_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc10dacf",
   "metadata": {},
   "source": [
    "Using logistic regression as the first classification model.\n",
    "\n",
    "First we are fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba96ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'outcome')\n",
    "\n",
    "lrModel = lr.fit(pipeline_df) # fit the logistic regression model to the training dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fe5c0a",
   "metadata": {},
   "source": [
    "Creating predicitions based on the anove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10e1d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(pipeline_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dec104",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5f5a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"rawPrediction\",\"probability\",\"prediction\",\"outcome\").toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74715b71",
   "metadata": {},
   "source": [
    "Test and Train accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d226c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = lrModel.transform(pipeline_df)# predictions using the training dataset\n",
    "accuracy_train = (predictions_train.filter(predictions_train.outcome == predictions_train.prediction)\n",
    "    .count() / float(predictions_train.count()))\n",
    "\n",
    "accuracy_test = (predictions.filter(predictions.outcome == predictions.prediction)\n",
    "    .count() / float(predictions.count()))\n",
    "print(f\"Train Accuracy : {np.round(accuracy_train*100,2)}%\")\n",
    "print(f\"Test Accuracy : {np.round(accuracy_test*100,2)}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb8b441",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb477cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c29acaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=[0.0,1.0,2.0,3.0,4.0,5.0,6.0]\n",
    "class_names_str=[\"normal\",\"slowite\", \"bruteforce\", \"flood\", \"malformed\",\"dos\",\"legitimate\"]\n",
    "\n",
    "outcome_true = predictions.select(\"outcome\")\n",
    "outcome_true = outcome_true.toPandas()\n",
    "\n",
    "pred = predictions.select(\"prediction\")\n",
    "pred = pred.toPandas()\n",
    "\n",
    "cnf_matrix = confusion_matrix(outcome_true, pred,labels=class_names)\n",
    "#cnf_matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names_str,\n",
    "                      title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4647a95",
   "metadata": {},
   "source": [
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc86a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='outcome', metricName='f1')\n",
    "print(\"Area under the curve/Accuracy is: \", evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68647f6",
   "metadata": {},
   "source": [
    "Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ecf5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'outcome')\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "lr_paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.0001, 0.001, 0.1])\n",
    "             .addGrid(lr.maxIter, [10, 100, 1000])\n",
    "             .build())\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='outcome', metricName='f1')\n",
    "\n",
    "# Create a CrossValidator for multi-class classification\n",
    "lr_cv = CrossValidator(estimator=lr, estimatorParamMaps=lr_paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Fit the CrossValidator to your data\n",
    "cv_model = lr_cv.fit(pipeline_df)\n",
    "\n",
    "# Make predictions on your test data\n",
    "predictions = cv_model.transform(pipeline_df_test)\n",
    "\n",
    "# Evaluate the model's performance using the F1 score or other relevant metrics\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fdc0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Area under the curve/Accuracy is: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6256a2",
   "metadata": {},
   "source": [
    "Random Forrest Classifier is our second classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d18a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'outcome')\n",
    "rf_model = rf.fit(pipeline_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bc0535",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(pipeline_df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8671086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df191a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"rawPrediction\",\"probability\",\"prediction\",\"outcome\").toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc911d88",
   "metadata": {},
   "source": [
    "Test and Train accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c55dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_prediction_train = rf_model.transform(pipeline_df)\n",
    "rf_prediction_test = rf_model.transform(pipeline_df_test)\n",
    "\n",
    "rf_accuracy_train = (rf_prediction_train.filter(rf_prediction_train.outcome == rf_prediction_train.prediction)\n",
    "    .count()/ float(rf_prediction_train.count()))\n",
    "rf_accuracy_test = (rf_prediction_test.filter(rf_prediction_test.outcome == rf_prediction_test.prediction)\n",
    "    .count() / float(rf_prediction_test.count()))\n",
    "\n",
    "rf_auc = evaluator.evaluate(rf_prediction_test)\n",
    "\n",
    "print(f\"Train accuracy = {np.round(rf_accuracy_train*100,2)}%, test accuracy = {np.round(rf_accuracy_test*100,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8caf497",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6711b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea9787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=[0.0,1.0,2.0,3.0,4.0,5.0,6.0]\n",
    "class_names_str=[\"normal\",\"slowite\", \"bruteforce\", \"flood\", \"malformed\",\"dos\",\"legitimate\"]\n",
    "\n",
    "outcome_true = predictions.select(\"outcome\")\n",
    "outcome_true = outcome_true.toPandas()\n",
    "\n",
    "pred = predictions.select(\"prediction\")\n",
    "pred = pred.toPandas()\n",
    "\n",
    "cnf_matrix = confusion_matrix(outcome_true, pred,labels=class_names)\n",
    "#cnf_matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names_str,\n",
    "                      title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6980ac00",
   "metadata": {},
   "source": [
    "Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d302fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [5, 10, 15])# maximum depth for each tree\n",
    "             .addGrid(rf.numTrees,[10, 20, 40])# number of trues\n",
    "             .build())\n",
    "\n",
    "rf_cv = CrossValidator(estimator=rf, estimatorParamMaps=rf_paramGrid, \n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "\n",
    "rf_cv_model = rf_cv.fit(pipeline_df)\n",
    "\n",
    "rf_cv_prediction_test = rf_cv_model.transform(pipeline_df_test)\n",
    "rf_cv_auc = evaluator.evaluate(rf_cv_prediction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b67ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before cross-validation and parameter tuning, AUC/accuracy={np.round(rf_auc,2)}\")\n",
    "print(f\"After cross-validation and parameter tuning, AUC/accuracy={np.round(rf_cv_auc,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1e6662",
   "metadata": {},
   "source": [
    "Pytorch ML modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f00f66",
   "metadata": {},
   "source": [
    "Creating the pipeline and splitting our dataset to validation_data and test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b11ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import Imputer, StandardScaler, StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Convert Spark DataFrames to Pandas DataFrames\n",
    "nslkdd_pd = pipeline_df.toPandas()\n",
    "nslkdd_test_pd = pipeline_df_test.toPandas()\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "# 50% of KDDTest+ for validation and the remaining 50% for testing\n",
    "split_ratio = 0.5\n",
    "split_index = int(len(nslkdd_test_pd) * split_ratio)\n",
    "\n",
    "validation_data = nslkdd_test_pd[:split_index]\n",
    "test_data = nslkdd_test_pd[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed32bbc",
   "metadata": {},
   "source": [
    "Creating tensors of train, test and validate datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e59313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.from_numpy(np.array(nslkdd_pd['features'].values.tolist(),np.float32))\n",
    "y_train = torch.from_numpy(np.array(nslkdd_pd['outcome'].values.tolist(),np.int64))\n",
    "x_validate = torch.from_numpy(np.array(validation_data['features'].values.tolist(),np.float32))\n",
    "y_validate = torch.from_numpy(np.array(validation_data['outcome'].values.tolist(),np.int64))\n",
    "x_test = torch.from_numpy(np.array(test_data['features'].values.tolist(),np.float32))\n",
    "y_test = torch.from_numpy(np.array(test_data['outcome'].values.tolist(),np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898cb691",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b46f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    def __getitem__(self,idx):\n",
    "        return (self.x[idx],self.y[idx])\n",
    "\n",
    "train_dataset = MyDataset(x_train,y_train)\n",
    "validate_dataset = MyDataset(x_validate,y_validate)\n",
    "test_dataset = MyDataset(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076f804d",
   "metadata": {},
   "source": [
    "Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff2cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class myMultilayerPerceptron(nn.Module):\n",
    "    def __init__(self,input_dm,output_dm):\n",
    "        super().__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(input_dm,60),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(60,30),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(30,15),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(15,7),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(7,output_dm)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        y = self.sequential(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2848b06",
   "metadata": {},
   "source": [
    "Initializing instance of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e368b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = myMultilayerPerceptron(x_train.shape[1],7)\n",
    "print(mymodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb731fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 0.1 \n",
    "batch_size = 64\n",
    "N_epochs = 10\n",
    "\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "# Adam Optimizer\n",
    "optimizer = torch.optim.Adam(mymodel.parameters(),lr = lr)\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "validate_losses = []\n",
    "validate_accuracies = []\n",
    "\n",
    "current_best_accuracy = 0.0\n",
    "\n",
    "import numpy as np\n",
    "for epoch in range(N_epochs):\n",
    "    \n",
    "    batch_loss = []\n",
    "    batch_accuracy = []\n",
    "    \n",
    "    for x_batch,y_batch in train_dataloader:\n",
    "        prediction_score = mymodel(x_batch)\n",
    "        loss = loss_fun(prediction_score,y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss.append(loss.detach().numpy())\n",
    "        prediction_label = torch.argmax(prediction_score.detach(),dim=1).numpy()\n",
    "        batch_accuracy.append(np.sum(prediction_label == y_batch.numpy())/x_batch.shape[0])\n",
    "    \n",
    "    validate_batch_loss = []\n",
    "    validate_batch_accuracy = []\n",
    "    \n",
    "    for x_batch,y_batch in validate_dataloader:\n",
    "        prediction_score = mymodel(x_batch)\n",
    "        loss = loss_fun(prediction_score,y_batch)\n",
    "        validate_batch_loss.append(loss.detach())\n",
    "        prediction_label = torch.argmax(prediction_score.detach(),dim=1).numpy()\n",
    "        validate_batch_accuracy.append(np.sum(prediction_label == y_batch.numpy())/x_batch.shape[0])\n",
    "    \n",
    "    losses.append(np.mean(np.array(batch_loss)))\n",
    "    validate_losses.append(np.mean(np.array(validate_batch_loss)))\n",
    "    \n",
    "    accuracies.append(np.mean(np.array(batch_accuracy)))\n",
    "    validate_accuracies.append(np.mean(np.array(validate_batch_accuracy)))\n",
    "    \n",
    "    print(f\"Epoch={epoch},train_loss={losses[-1]},validate_loss={validate_losses[-1]}\")\n",
    "    print(f\"Train_accuracy={np.round(accuracies[-1]*100,2)}%,validate_accuracy={np.round(validate_accuracies[-1]*100,2)}%\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if validate_accuracies[-1]>current_best_accuracy:\n",
    "        print(\"Current epoch is best so far, saving model...\")\n",
    "        torch.save(mymodel.state_dict(),'current_best_model')\n",
    "        current_best_accuracy = validate_accuracies[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6305aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Reshape losses and accuracies arrays to match the total number of train iterations\n",
    "train_loss_per_iteration = np.array(batch_loss).reshape(-1)\n",
    "\n",
    "# Create the plots\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: The train loss per SGD iteration\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_loss_per_iteration)\n",
    "plt.xlabel('SGD Iteration')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.title('Train Loss per SGD Iteration')\n",
    "\n",
    "# Plot 2: The train and validate loss across different epochs\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(np.arange(1, N_epochs+1), losses, label='Train Loss')\n",
    "plt.plot(np.arange(1, N_epochs+1), validate_losses, label='Validate Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Validate Loss across Different Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 3: The train and validate metric across different epochs\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(np.arange(1, N_epochs+1), accuracies, label='Train Accuracy')\n",
    "plt.plot(np.arange(1, N_epochs+1), validate_accuracies, label='Validate Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Validate Accuracy across Different Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae102ee",
   "metadata": {},
   "source": [
    "Hypertuning it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505121b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 0.05 \n",
    "batch_size = 128\n",
    "N_epochs = 10\n",
    "\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "# Adam Optimizer\n",
    "optimizer = torch.optim.Adam(mymodel.parameters(),lr = lr)\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "validate_losses = []\n",
    "validate_accuracies = []\n",
    "\n",
    "current_best_accuracy = 0.0\n",
    "\n",
    "import numpy as np\n",
    "for epoch in range(N_epochs):\n",
    "    \n",
    "    batch_loss = []\n",
    "    batch_accuracy = []\n",
    "    \n",
    "    for x_batch,y_batch in train_dataloader:\n",
    "        prediction_score = mymodel(x_batch)\n",
    "        loss = loss_fun(prediction_score,y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss.append(loss.detach().numpy())\n",
    "        prediction_label = torch.argmax(prediction_score.detach(),dim=1).numpy()\n",
    "        batch_accuracy.append(np.sum(prediction_label == y_batch.numpy())/x_batch.shape[0])\n",
    "    \n",
    "    validate_batch_loss = []\n",
    "    validate_batch_accuracy = []\n",
    "    \n",
    "    for x_batch,y_batch in validate_dataloader:\n",
    "        prediction_score = mymodel(x_batch)\n",
    "        loss = loss_fun(prediction_score,y_batch)\n",
    "        validate_batch_loss.append(loss.detach())\n",
    "        prediction_label = torch.argmax(prediction_score.detach(),dim=1).numpy()\n",
    "        validate_batch_accuracy.append(np.sum(prediction_label == y_batch.numpy())/x_batch.shape[0])\n",
    "    \n",
    "    losses.append(np.mean(np.array(batch_loss)))\n",
    "    validate_losses.append(np.mean(np.array(validate_batch_loss)))\n",
    "    \n",
    "    accuracies.append(np.mean(np.array(batch_accuracy)))\n",
    "    validate_accuracies.append(np.mean(np.array(validate_batch_accuracy)))\n",
    "    \n",
    "    print(f\"Epoch={epoch},train_loss={losses[-1]},validate_loss={validate_losses[-1]}\")\n",
    "    print(f\"Train_accuracy={np.round(accuracies[-1]*100,2)}%,validate_accuracy={np.round(validate_accuracies[-1]*100,2)}%\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if validate_accuracies[-1]>current_best_accuracy:\n",
    "        print(\"Current epoch is best so far, saving model...\")\n",
    "        torch.save(mymodel.state_dict(),'current_best_model')\n",
    "        current_best_accuracy = validate_accuracies[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35de37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Reshape losses and accuracies arrays to match the total number of train iterations\n",
    "train_loss_per_iteration = np.array(batch_loss).reshape(-1)\n",
    "\n",
    "# Create the plots\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: The train loss per SGD iteration\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_loss_per_iteration)\n",
    "plt.xlabel('SGD Iteration')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.title('Train Loss per SGD Iteration')\n",
    "\n",
    "# Plot 2: The train and validate loss across different epochs\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(np.arange(1, N_epochs+1), losses, label='Train Loss')\n",
    "plt.plot(np.arange(1, N_epochs+1), validate_losses, label='Validate Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Validate Loss across Different Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 3: The train and validate metric across different epochs\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(np.arange(1, N_epochs+1), accuracies, label='Train Accuracy')\n",
    "plt.plot(np.arange(1, N_epochs+1), validate_accuracies, label='Validate Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Validate Accuracy across Different Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cba5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "mybestmodel = myMultilayerPerceptron(x_train.shape[1],7)\n",
    "mybestmodel.load_state_dict(torch.load(\"current_best_model\"))\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_batch_accuracy = []\n",
    "\n",
    "for x_batch, y_batch in test_dataloader:\n",
    "    prediction_score = mybestmodel(x_batch)\n",
    "    prediction_label = torch.argmax(prediction_score.detach(),dim=1).numpy()\n",
    "    test_batch_accuracy.append(np.sum(prediction_label == y_batch.numpy())/x_batch.shape[0])\n",
    "\n",
    "test_accuracy = np.mean(np.array(test_batch_accuracy))\n",
    "\n",
    "print(f\"Test accuracy = {np.round(test_accuracy*100,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b9daf7",
   "metadata": {},
   "source": [
    "Shallow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2dca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class myMultilayerPerceptron(nn.Module):\n",
    "    def __init__(self,input_dm,output_dm):\n",
    "        super().__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(input_dm,30),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(30,15),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(15,output_dm)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        y = self.sequential(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf6163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = myMultilayerPerceptron(x_train.shape[1],7)\n",
    "print(mymodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d4f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 0.05 \n",
    "batch_size = 64\n",
    "N_epochs = 10\n",
    "\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "# Adam Optimizer\n",
    "optimizer = torch.optim.Adam(mymodel.parameters(),lr = lr)\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "validate_losses = []\n",
    "validate_accuracies = []\n",
    "\n",
    "current_best_accuracy = 0.0\n",
    "\n",
    "import numpy as np\n",
    "for epoch in range(N_epochs):\n",
    "    \n",
    "    batch_loss = []\n",
    "    batch_accuracy = []\n",
    "    \n",
    "    for x_batch,y_batch in train_dataloader:\n",
    "        prediction_score = mymodel(x_batch)\n",
    "        loss = loss_fun(prediction_score,y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss.append(loss.detach().numpy())\n",
    "        prediction_label = torch.argmax(prediction_score.detach(),dim=1).numpy()\n",
    "        batch_accuracy.append(np.sum(prediction_label == y_batch.numpy())/x_batch.shape[0])\n",
    "    \n",
    "    validate_batch_loss = []\n",
    "    validate_batch_accuracy = []\n",
    "    \n",
    "    for x_batch,y_batch in validate_dataloader:\n",
    "        prediction_score = mymodel(x_batch)\n",
    "        loss = loss_fun(prediction_score,y_batch)\n",
    "        validate_batch_loss.append(loss.detach())\n",
    "        prediction_label = torch.argmax(prediction_score.detach(),dim=1).numpy()\n",
    "        validate_batch_accuracy.append(np.sum(prediction_label == y_batch.numpy())/x_batch.shape[0])\n",
    "    \n",
    "    losses.append(np.mean(np.array(batch_loss)))\n",
    "    validate_losses.append(np.mean(np.array(validate_batch_loss)))\n",
    "    \n",
    "    accuracies.append(np.mean(np.array(batch_accuracy)))\n",
    "    validate_accuracies.append(np.mean(np.array(validate_batch_accuracy)))\n",
    "    \n",
    "    print(f\"Epoch={epoch},train_loss={losses[-1]},validate_loss={validate_losses[-1]}\")\n",
    "    print(f\"Train_accuracy={np.round(accuracies[-1]*100,2)}%,validate_accuracy={np.round(validate_accuracies[-1]*100,2)}%\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if validate_accuracies[-1]>current_best_accuracy:\n",
    "        print(\"Current epoch is best so far, saving model...\")\n",
    "        torch.save(mymodel.state_dict(),'current_best_model')\n",
    "        current_best_accuracy = validate_accuracies[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d2992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 0.001 \n",
    "batch_size = 128\n",
    "N_epochs = 50\n",
    "\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "# Adam Optimizer\n",
    "optimizer = torch.optim.Adam(mymodel.parameters(),lr = lr)\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "validate_losses = []\n",
    "validate_accuracies = []\n",
    "\n",
    "current_best_accuracy = 0.0\n",
    "\n",
    "import numpy as np\n",
    "for epoch in range(N_epochs):\n",
    "    \n",
    "    batch_loss = []\n",
    "    batch_accuracy = []\n",
    "    \n",
    "    for x_batch,y_batch in train_dataloader:\n",
    "        prediction_score = mymodel(x_batch)\n",
    "        loss = loss_fun(prediction_score,y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss.append(loss.detach().numpy())\n",
    "        prediction_label = torch.argmax(prediction_score.detach(),dim=1).numpy()\n",
    "        batch_accuracy.append(np.sum(prediction_label == y_batch.numpy())/x_batch.shape[0])\n",
    "    \n",
    "    validate_batch_loss = []\n",
    "    validate_batch_accuracy = []\n",
    "    \n",
    "    for x_batch,y_batch in validate_dataloader:\n",
    "        prediction_score = mymodel(x_batch)\n",
    "        loss = loss_fun(prediction_score,y_batch)\n",
    "        validate_batch_loss.append(loss.detach())\n",
    "        prediction_label = torch.argmax(prediction_score.detach(),dim=1).numpy()\n",
    "        validate_batch_accuracy.append(np.sum(prediction_label == y_batch.numpy())/x_batch.shape[0])\n",
    "    \n",
    "    losses.append(np.mean(np.array(batch_loss)))\n",
    "    validate_losses.append(np.mean(np.array(validate_batch_loss)))\n",
    "    \n",
    "    accuracies.append(np.mean(np.array(batch_accuracy)))\n",
    "    validate_accuracies.append(np.mean(np.array(validate_batch_accuracy)))\n",
    "    \n",
    "    print(f\"Epoch={epoch},train_loss={losses[-1]},validate_loss={validate_losses[-1]}\")\n",
    "    print(f\"Train_accuracy={np.round(accuracies[-1]*100,2)}%,validate_accuracy={np.round(validate_accuracies[-1]*100,2)}%\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if validate_accuracies[-1]>current_best_accuracy:\n",
    "        print(\"Current epoch is best so far, saving model...\")\n",
    "        torch.save(mymodel.state_dict(),'current_best_model')\n",
    "        current_best_accuracy = validate_accuracies[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb32d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "mybestmodel = myMultilayerPerceptron(x_train.shape[1],7)\n",
    "mybestmodel.load_state_dict(torch.load(\"current_best_model\"))\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_batch_accuracy = []\n",
    "\n",
    "for x_batch, y_batch in test_dataloader:\n",
    "    prediction_score = mybestmodel(x_batch)\n",
    "    prediction_label = torch.argmax(prediction_score.detach(),dim=1).numpy()\n",
    "    test_batch_accuracy.append(np.sum(prediction_label == y_batch.numpy())/x_batch.shape[0])\n",
    "\n",
    "test_accuracy = np.mean(np.array(test_batch_accuracy))\n",
    "\n",
    "print(f\"Test accuracy = {np.round(test_accuracy*100,2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c1968b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
